import streamlit as st
import requests
import pandas as pd
import plotly.graph_objects as go
import time
from datetime import datetime
import json
import concurrent.futures
import numpy as np

# ì§€ì—­ ì½”ë“œ ë§¤í•‘
REGION_CODES = {
    'A000': 'ì„œìš¸',
    'H000': 'ë¶€ì‚°', 
    'I000': 'ëŒ€êµ¬',
    'C000': 'ì¸ì²œ',
    'D000': 'ê´‘ì£¼',
    'E000': 'ëŒ€ì „',
    'F000': 'ìš¸ì‚°',
    'G000': 'ì„¸ì¢…',
    'B000': 'ê²½ê¸°',
    'J000': 'ê°•ì›',
    'K000': 'ì¶©ë¶',
    'L000': 'ì¶©ë‚¨',
    'M000': 'ì „ë¶',
    'N000': 'ì „ë‚¨',
    'O000': 'ê²½ë¶',
    'P000': 'ê²½ë‚¨',
    'Q000': 'ì œì£¼'
}

class RegionalAnalyzer:
    def __init__(self):
        self.base_url = 'https://bff-general.albamon.com'
        self.headers = {
            'Accept': '*/*',
            'User-Agent': 'job-site-monitor/1.0.0',
            'origin': 'https://www.albamon.com',
            'Content-Type': 'application/json',
            'cookie': 'ConditionId=25C99562-77E3-40EB-A750-DA27D2D03C54; ab.storage.deviceId.7a5f1472-069a-4372-8631-2f711442ee40=%7B%22g%22%3A%22efb20921-d9c8-43dd-3c27-8a1487d7d2c4%22%2C%22c%22%3A1756907811760%2C%22l%22%3A1756943038663%7D; AM_USER_UUID=e69544f8-bed4-4fc3-94c7-6efac20359f7; ab.storage.sessionId.7a5f1472-069a-4372-8631-2f711442ee40=%7B%22g%22%3A%22898147fc-a8ba-6427-1f60-647c10d3514e%22%2C%22e%22%3A1756945521947%2C%22c%22%3A1756943038661%2C%22l%22%3A1756943721947%7D'
        }
        # ê³ ê¸‰ ìºì‹œ ì‹œìŠ¤í…œ
        self._cache = {}
        self._cache_timeout = 300  # 5ë¶„ ìºì‹œ
        self._request_session = requests.Session()  # ì—°ê²° ì¬ì‚¬ìš©
        self._request_session.headers.update(self.headers)
        # ì„±ëŠ¥ ì¹´ìš´í„°
        self.performance_stats = {
            'api_calls': 0,
            'cache_hits': 0,
            'total_processing_time': 0
        }
    
    def _get_cache_key(self, region_code, search_period_type, max_pages):
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"{region_code}_{search_period_type}_{max_pages}_{datetime.now().strftime('%H_%M')[:4]}"  # 10ë¶„ ë‹¨ìœ„ë¡œ ìºì‹œ
    
    def _get_from_cache(self, cache_key):
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        if cache_key in self._cache:
            cached_data, timestamp = self._cache[cache_key]
            if time.time() - timestamp < self._cache_timeout:
                self.performance_stats['cache_hits'] += 1
                return cached_data
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self._cache[cache_key]
        return None
    
    def _set_cache(self, cache_key, data):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        self._cache[cache_key] = (data, time.time())

    def categorize_job_posting(self, job):
        """
        ê³µê³  ì†ŒìŠ¤ ë° ìœ í˜• ë¶„ë¥˜
        """
        # ì†ŒìŠ¤ ë¶„ë¥˜
        if job.get('jobkoreaRecruitNo', 0) != 0:
            source = 'JOBKOREA'
        elif job.get('externalRecruitSite') == 'WN':
            source = 'WORKNET'
        else:
            source = 'ALBAMON'
        
        # ìœ ë£Œ/ë¬´ë£Œ ë¶„ë¥˜ (ALBAMON ê³µê³ ë§Œ)
        is_paid = False
        product_count = 0
        if source == 'ALBAMON':
            product_count = job.get('paidService', {}).get('totalProductCount', 0)
            is_paid = product_count > 0
        
        return {
            'source': source,
            'is_paid': is_paid,
            'product_count': product_count
        }

    def search_regional_jobs(self, region_code, page=1, size=200, search_period_type='ALL'):
        """
        ì§€ì—­ë³„ ê³µê³  ê²€ìƒ‰
        """
        request_body = {
            "pagination": {
                "page": int(page),
                "size": int(size)
            },
            "recruitListType": "AREA",
            "sortTabCondition": {
                "searchPeriodType": str(search_period_type),
                "sortType": "DEFAULT"
            },
            "condition": {
                "areas": [
                    {
                        "si": region_code,
                        "gu": "",
                        "dong": ""
                    }
                ],
                "employmentTypes": [],
                "excludeKeywords": [],
                "excludeBar": False,
                "excludeNegoAge": False,
                "excludeNegoWorkWeek": False,
                "excludeNegoWorkTime": False,
                "excludeNegoGender": False,
                "parts": [],
                "similarDongJoin": False,
                "workDayTypes": [],
                "workPeriodTypes": [],
                "workTimeTypes": [],
                "workWeekTypes": [],
                "endWorkTime": "",
                "startWorkTime": "",
                "includeKeyword": "",
                "excludeKeywordList": [],
                "age": 0,
                "genderType": "NONE",
                "moreThanEducation": False,
                "educationType": "ALL",
                "selectedArea": {
                    "si": "",
                    "gu": "",
                    "dong": ""
                }
            }
        }
        
        try:
            # ì„¸ì…˜ ê¸°ë°˜ ìš”ì²­ìœ¼ë¡œ ì—°ê²° ì¬ì‚¬ìš© (ì†ë„ í–¥ìƒ)
            response = self._request_session.post(
                f'{self.base_url}/recruit/search',
                json=request_body,
                timeout=15  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
            )
            response.raise_for_status()
            data = response.json()
            self.performance_stats['api_calls'] += 1
            
            # ì˜¬ë°”ë¥¸ JSON ê²½ë¡œë¡œ ê³µê³  ë°ì´í„° ì¶”ì¶œ
            jobs = data.get('base', {}).get('normal', {}).get('collection', [])
            
            return {
                'result': {'recruitList': jobs},
                'base': data.get('base', {}),
                '_debug_info': {
                    'original_job_count': len(jobs),
                    'region_code': region_code,
                    'json_structure': 'base.normal.collection'
                }
            }
        except requests.exceptions.RequestException as e:
            st.error(f"ì§€ì—­ë³„ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None

    def fetch_page_data(self, region_code, page, size, search_period_type):
        """ë‹¨ì¼ í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            response = self.search_regional_jobs(region_code, page, size, search_period_type)
            if response:
                jobs = response.get('result', {}).get('recruitList', [])
                return {'page': page, 'jobs': jobs, 'success': True}
            return {'page': page, 'jobs': [], 'success': False}
        except Exception as e:
            st.error(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            return {'page': page, 'jobs': [], 'success': False}

    def analyze_regional_jobs(self, region_code, region_name, search_period_type='ALL', max_pages=3):
        """
        ì§€ì—­ë³„ ê³µê³  ë¶„ì„ (ìœ ë£Œ/ë¬´ë£Œ í¬í•¨) - ìµœì í™”ëœ ë²„ì „
        """
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._get_cache_key(region_code, search_period_type, max_pages)
            cached_result = self._get_from_cache(cache_key)
            if cached_result:
                st.success("âš¡ ìºì‹œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (5ë¶„ ìºì‹œ)")
                return cached_result
            # ì²« ë²ˆì§¸ í˜ì´ì§€ë¡œ ì „ì²´ ê³µê³  ìˆ˜ í™•ì¸
            first_response = self.search_regional_jobs(region_code, 1, 200, search_period_type)
            if not first_response:
                return None
                
            total_count = first_response.get('base', {}).get('pagination', {}).get('totalCount', 0)
            calculated_max_pages = (total_count + 199) // 200 if total_count > 0 else 1
            actual_max_pages = min(max_pages, calculated_max_pages)
            
            st.info(f"{region_name} ì „ì²´ ê³µê³  ìˆ˜: {total_count:,}ê°œ ({calculated_max_pages} í˜ì´ì§€)")
            st.info(f"ë¶„ì„ ëŒ€ìƒ: {actual_max_pages}í˜ì´ì§€ (ìƒ˜í”Œë§)")
            
            if total_count == 0:
                return {
                    'region_name': region_name,
                    'region_code': region_code,
                    'total_count': 0,
                    'albamon_count': 0,
                    'albamon_free_count': 0,
                    'albamon_paid_count': 0,
                    'jobkorea_count': 0,
                    'worknet_count': 0,
                    'sample_jobs': []
                }
            
            # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ í˜ì´ì§€ ë¶„ì„ (ì†ë„ ëŒ€í­ ê°œì„ )
            all_jobs = []
            start_time = time.time()
            
            # ë” ë§ì€ ë™ì‹œ ì—°ê²° í—ˆìš© (ì†ë„ ëŒ€í­ í–¥ìƒ)
            max_workers = min(actual_max_pages, 10)  # 5 â†’ 10ìœ¼ë¡œ ì¦ê°€
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ìš”ì²­
                future_to_page = {
                    executor.submit(self.fetch_page_data, region_code, page, 200, search_period_type): page 
                    for page in range(1, actual_max_pages + 1)
                }
                
                # ì§„í–‰ ìƒí™© í‘œì‹œìš©
                completed_count = 0
                progress_placeholder = st.empty()
                
                for future in concurrent.futures.as_completed(future_to_page):
                    page = future_to_page[future]
                    result = future.result()
                    
                    completed_count += 1
                    progress_placeholder.info(f"ğŸ“¡ API í˜¸ì¶œ ì§„í–‰ ì¤‘... {completed_count}/{actual_max_pages} í˜ì´ì§€ ì™„ë£Œ")
                    
                    if result['success']:
                        all_jobs.extend(result['jobs'])
            
            elapsed_time = time.time() - start_time
            st.success(f"âš¡ {actual_max_pages}í˜ì´ì§€ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ({elapsed_time:.1f}ì´ˆ)")
            progress_placeholder.empty()
            
            # ì´ˆê³ ì† ë²¡í„°í™” ë¶„ë¥˜ ì²˜ë¦¬
            start_classification = time.time()
            
            if not all_jobs:
                counters = {'albamon_count': 0, 'albamon_free_count': 0, 'albamon_paid_count': 0, 'jobkorea_count': 0, 'worknet_count': 0}
                sample_jobs = []
            else:
                # NumPy ë²¡í„°í™”ë¡œ ì´ˆê³ ì† ì²˜ë¦¬
                job_count = len(all_jobs)
                
                # ë²¡í„°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì¶”ì¶œ
                jobkorea_nos = np.array([job.get('jobkoreaRecruitNo', 0) for job in all_jobs])
                external_sites = np.array([job.get('externalRecruitSite', '') for job in all_jobs])
                product_counts = np.array([job.get('paidService', {}).get('totalProductCount', 0) for job in all_jobs])
                
                # ë²¡í„°í™”ëœ ë¶„ë¥˜
                is_jobkorea = jobkorea_nos != 0
                is_worknet = (external_sites == 'WN') & ~is_jobkorea  # JOBKOREAê°€ ìš°ì„ 
                is_albamon = ~is_jobkorea & ~is_worknet
                is_paid = is_albamon & (product_counts > 0)
                is_free = is_albamon & (product_counts == 0)
                
                # ë¹ ë¥¸ ì¹´ìš´íŒ…
                counters = {
                    'jobkorea_count': int(np.sum(is_jobkorea)),
                    'worknet_count': int(np.sum(is_worknet)),
                    'albamon_count': int(np.sum(is_albamon)),
                    'albamon_paid_count': int(np.sum(is_paid)),
                    'albamon_free_count': int(np.sum(is_free))
                }
                
                # ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œë§Œ, ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
                sample_jobs = []
                for i in range(min(10, job_count)):
                    job = all_jobs[i]
                    source = 'JOBKOREA' if is_jobkorea[i] else 'WORKNET' if is_worknet[i] else 'ALBAMON'
                    sample_jobs.append({
                        'recruitNo': job.get('recruitNo'),
                        'title': job.get('recruitTitle', '')[:40] + '...',
                        'source': source,
                        'is_paid': bool(is_paid[i]) if is_albamon[i] else False,
                        'product_count': int(product_counts[i]) if is_albamon[i] else 0,
                        'pay': job.get('pay', ''),
                        'workplaceArea': job.get('workplaceArea', ''),
                        'jobkoreaRecruitNo': int(jobkorea_nos[i]),
                        'externalRecruitSite': external_sites[i],
                        'paidService': str(job.get('paidService', {}))
                    })
            
            classification_time = time.time() - start_classification
            st.info(f"ğŸ“Š {len(all_jobs):,}ê°œ ê³µê³  ë¶„ë¥˜ ì™„ë£Œ ({classification_time:.2f}ì´ˆ)")
            
            # ì¹´ìš´í„°ì—ì„œ ê°’ ì¶”ì¶œ
            albamon_count = counters['albamon_count']
            albamon_free_count = counters['albamon_free_count'] 
            albamon_paid_count = counters['albamon_paid_count']
            jobkorea_count = counters['jobkorea_count']
            worknet_count = counters['worknet_count']
            
            # ì™¸ë¶€ ì—°ë™ ê³µê³ ê°€ ìˆì„ ë•Œë§Œ ê°„ë‹¨íˆ í‘œì‹œ
            external_count = jobkorea_count + worknet_count
            if external_count > 0:
                st.success(f"ğŸ”— ì™¸ë¶€ ì—°ë™ ê³µê³  {external_count:,}ê°œ ë°œê²¬ (ì¡ì½”ë¦¬ì•„: {jobkorea_count:,}ê°œ, ì›Œí¬ë„·: {worknet_count:,}ê°œ)")
            
            # ì§‘ê³„ ì˜¤ë¥˜ ê²€ì¦ (ì˜¤ë¥˜ ì‹œì—ë§Œ í‘œì‹œ)
            if albamon_free_count + albamon_paid_count != albamon_count:
                st.error(f"âš ï¸ ì§‘ê³„ ì˜¤ë¥˜ ë°œê²¬ - ìˆ˜ì • ì¤‘...")
            
            # ë¹„ìœ¨ì— ë”°ë¥¸ ì „ì²´ ì¶”ì • (ë””ë²„ê¹… ì •ë³´ ìµœì†Œí™”)
            if len(all_jobs) > 0 and len(all_jobs) < total_count:
                # ìƒ˜í”Œ ë¹„ìœ¨ë¡œ ì „ì²´ ì¶”ì •
                ratio = total_count / len(all_jobs)
                
                albamon_estimated = int(albamon_count * ratio)
                albamon_free_estimated = int(albamon_free_count * ratio)
                albamon_paid_estimated = int(albamon_paid_count * ratio)
                jobkorea_estimated = int(jobkorea_count * ratio)
                worknet_estimated = int(worknet_count * ratio)
                
                # ì¶”ì • ì™„ë£Œ ì•Œë¦¼ë§Œ í‘œì‹œ
                st.info(f"ğŸ“Š ìƒ˜í”Œ {len(all_jobs):,}ê°œ ë¶„ì„ â†’ ì „ì²´ {total_count:,}ê°œ ì¶”ì • ì™„ë£Œ")
                    
            else:
                # ì „ì²´ ë¶„ì„ ì™„ë£Œ
                albamon_estimated = albamon_count
                albamon_free_estimated = albamon_free_count
                albamon_paid_estimated = albamon_paid_count
                jobkorea_estimated = jobkorea_count
                worknet_estimated = worknet_count
            
            # ìµœì¢… ê²€ì¦ (ì˜¤ë¥˜ ì‹œì—ë§Œ ìë™ ìˆ˜ì •)
            if albamon_free_estimated + albamon_paid_estimated != albamon_estimated:
                albamon_estimated = albamon_free_estimated + albamon_paid_estimated
            
            # ê²°ê³¼ ìƒì„±
            result = {
                'region_name': region_name,
                'region_code': region_code,
                'total_count': total_count,
                'analyzed_count': len(all_jobs),
                'albamon_count': albamon_estimated,
                'albamon_free_count': albamon_free_estimated,
                'albamon_paid_count': albamon_paid_estimated,
                'jobkorea_count': jobkorea_estimated,
                'worknet_count': worknet_estimated,
                'sample_jobs': sample_jobs,
                'sample_stats': {
                    'albamon_sample': albamon_count,
                    'albamon_free_sample': albamon_free_count,
                    'albamon_paid_sample': albamon_paid_count,
                    'jobkorea_sample': jobkorea_count,
                    'worknet_sample': worknet_count
                },
                'performance': {
                    'api_time': elapsed_time,
                    'classification_time': classification_time,
                    'total_jobs_processed': len(all_jobs),
                    'api_calls_made': self.performance_stats['api_calls'],
                    'cache_hits': self.performance_stats['cache_hits'],
                    'avg_time_per_page': elapsed_time / max(actual_max_pages, 1),
                    'processing_speed': len(all_jobs) / max(classification_time, 0.001),
                    'concurrent_workers': max_workers
                }
            }
            
            # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            self._set_cache(cache_key, result)
            st.success(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ ìºì‹œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (5ë¶„ê°„ ìœ íš¨)")
            
            return result
            
        except Exception as e:
            st.error(f"ì§€ì—­ë³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

def render_regional_dashboard(results):
    """ì§€ì—­ë³„ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    if not results or results['total_count'] == 0:
        st.info("í•´ë‹¹ ì§€ì—­ì— ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    st.header(f"ğŸ™ï¸ {results['region_name']} ê³µê³  ë¶„ì„ ê²°ê³¼")
    
    # ë©”íŠ¸ë¦­ ì¹´ë“œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="ğŸ“Š ì „ì²´ ê³µê³  ìˆ˜",
            value=f"{results['total_count']:,}ê°œ"
        )
    
    with col2:
        st.metric(
            label="ğŸ¢ ì•Œë°”ëª¬ ìì‚¬",
            value=f"{results['albamon_count']:,}ê°œ",
            delta=f"{(results['albamon_count']/results['total_count']*100):.1f}%"
        )
    
    with col3:
        st.metric(
            label="ğŸ†“ ë¬´ë£Œ ê³µê³ ",
            value=f"{results['albamon_free_count']:,}ê°œ",
            delta=f"{(results['albamon_free_count']/results['total_count']*100):.1f}%"
        )
    
    with col4:
        st.metric(
            label="ğŸ”— ì™¸ë¶€ ì—°ë™",
            value=f"{results['jobkorea_count'] + results['worknet_count']:,}ê°œ",
            delta=f"{((results['jobkorea_count'] + results['worknet_count'])/results['total_count']*100):.1f}%"
        )
    
    # ì°¨íŠ¸ ì„¹ì…˜
    col1, col2 = st.columns(2)
    
    with col1:
        # ì†ŒìŠ¤ë³„ íŒŒì´ ì°¨íŠ¸
        fig_source = go.Figure(data=[go.Pie(
            labels=['ì•Œë°”ëª¬ ìì‚¬', 'ì¡ì½”ë¦¬ì•„', 'ì›Œí¬ë„·'],
            values=[results['albamon_count'], results['jobkorea_count'], results['worknet_count']],
            hole=.3,
            marker_colors=['#FF6B6B', '#4ECDC4', '#45B7D1']
        )])
        fig_source.update_layout(title="ì†ŒìŠ¤ë³„ ë¶„í¬")
        st.plotly_chart(fig_source, use_container_width=True)
    
    with col2:
        # ë°” ì°¨íŠ¸
        fig_bar = go.Figure(data=[
            go.Bar(
                x=['ë¬´ë£Œ ê³µê³ ', 'ìœ ë£Œ ê³µê³ ', 'ì¡ì½”ë¦¬ì•„', 'ì›Œí¬ë„·'],
                y=[results['albamon_free_count'], results['albamon_paid_count'], 
                   results['jobkorea_count'], results['worknet_count']],
                marker_color=['#95E1D3', '#F38BA8', '#4ECDC4', '#45B7D1']
            )
        ])
        fig_bar.update_layout(title="ê³µê³  ìœ í˜•ë³„ ë¹„êµ", yaxis_title="ê³µê³  ìˆ˜")
        st.plotly_chart(fig_bar, use_container_width=True)
    
    # ìƒì„¸ ì •ë³´ í…Œì´ë¸”
    st.subheader("ğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
    detail_data = {
        'êµ¬ë¶„': ['ì•Œë°”ëª¬ ìì‚¬', '  â”” ë¬´ë£Œ ê³µê³ ', '  â”” ìœ ë£Œ ê³µê³ ', 'ì¡ì½”ë¦¬ì•„', 'ì›Œí¬ë„·', 'ì „ì²´'],
        'ê³µê³  ìˆ˜': [
            f"{results['albamon_count']:,}",
            f"{results['albamon_free_count']:,}",
            f"{results['albamon_paid_count']:,}",
            f"{results['jobkorea_count']:,}",
            f"{results['worknet_count']:,}",
            f"{results['total_count']:,}"
        ],
        'ë¹„ìœ¨ (%)': [
            f"{results['albamon_count']/results['total_count']*100:.2f}",
            f"{results['albamon_free_count']/results['total_count']*100:.2f}",
            f"{results['albamon_paid_count']/results['total_count']*100:.2f}",
            f"{results['jobkorea_count']/results['total_count']*100:.2f}",
            f"{results['worknet_count']/results['total_count']*100:.2f}",
            "100.00"
        ]
    }
    
    df_detail = pd.DataFrame(detail_data)
    st.dataframe(df_detail, use_container_width=True)
    
    # ìƒ˜í”Œ ê³µê³  ë°ì´í„°
    if results['sample_jobs']:
        st.subheader("ğŸ“‹ ìƒ˜í”Œ ê³µê³  ë°ì´í„°")
        st.write(f"ë¶„ì„ëœ ìƒ˜í”Œ: {results['analyzed_count']:,}ê°œ ê³µê³  ì¤‘ ìƒìœ„ 10ê°œ")
        
        sample_df = pd.DataFrame(results['sample_jobs'])
        st.dataframe(sample_df, use_container_width=True)
    
    # JSON ë‹¤ìš´ë¡œë“œ
    st.download_button(
        label="ğŸ“¥ ê²°ê³¼ JSON ë‹¤ìš´ë¡œë“œ",
        data=json.dumps(results, indent=2, ensure_ascii=False),
        file_name=f"{results['region_name']}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

def main():
    st.set_page_config(
        page_title="ì§€ì—­ë³„ ê³µê³  ë¶„ì„",
        page_icon="ğŸ™ï¸",
        layout="wide"
    )

    st.title("ğŸ™ï¸ ì§€ì—­ë³„ ê³µê³  ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("ì§€ì—­ë³„ ë¬´ë£Œ/ìœ ë£Œ ê³µê³  í˜„í™©ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

    analyzer = RegionalAnalyzer()

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ™ï¸ ì§€ì—­ ì„ íƒ")
        
        selected_region_code = st.selectbox(
            "ë¶„ì„í•  ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”",
            options=list(REGION_CODES.keys()),
            format_func=lambda x: f"{REGION_CODES[x]} ({x})"
        )
        
        period_type = st.selectbox(
            "ê¸°ê°„ ì„ íƒ",
            options=['ALL', 'TODAY'],
            format_func=lambda x: "ì „ì²´" if x == 'ALL' else "ì˜¤ëŠ˜"
        )
        
        max_pages = st.slider(
            "ë¶„ì„í•  í˜ì´ì§€ ìˆ˜ (ìƒ˜í”Œë§)",
            min_value=1,
            max_value=10,
            value=3,
            help="ë” ë§ì€ í˜ì´ì§€ë¥¼ ë¶„ì„í• ìˆ˜ë¡ ì •í™•ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤."
        )
        
        if st.button("ğŸ” ì§€ì—­ë³„ ë¶„ì„ ì‹œì‘", type="primary"):
            st.session_state.run_regional_analysis = True
            st.session_state.selected_region = selected_region_code
            st.session_state.selected_period = period_type
            st.session_state.selected_max_pages = max_pages

    # ì§€ì—­ë³„ ë¶„ì„ ì‹¤í–‰
    if hasattr(st.session_state, 'run_regional_analysis') and st.session_state.run_regional_analysis:
        region_code = st.session_state.selected_region
        region_name = REGION_CODES[region_code]
        period_type = st.session_state.selected_period
        max_pages = st.session_state.selected_max_pages
        
        with st.spinner(f"{region_name} ì§€ì—­ ê³µê³ ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            results = analyzer.analyze_regional_jobs(
                region_code, 
                region_name, 
                period_type, 
                max_pages
            )
        
        if results:
            render_regional_dashboard(results)
        
        st.session_state.run_regional_analysis = False

    # í‘¸í„°
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
    <small>ì§€ì—­ë³„ ê³µê³  ë¶„ì„ ëŒ€ì‹œë³´ë“œ | ë¬´ë£Œ/ìœ ë£Œ ê³µê³  êµ¬ë¶„ ê¸°ëŠ¥ í¬í•¨</small>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()